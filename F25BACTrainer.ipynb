{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJu8OL4lGUKE"
      },
      "outputs": [],
      "source": [
        "# Reinstall specific compatible versions (transformers provides PreTrainedModel export)\n",
        "!pip uninstall -y transformers peft trl\n",
        "!pip install transformers==4.55.0 peft==0.18.0 trl accelerate bitsandbytes datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostics: versions and PreTrainedModel availability\n",
        "import sys, pkgutil, importlib\n",
        "import torch, transformers\n",
        "print('Python:', sys.version)\n",
        "print('Torch:', torch.__version__)\n",
        "print('Transformers:', transformers.__version__, '->', transformers.__file__)\n",
        "print('Has PreTrainedModel attr:', hasattr(transformers,'PreTrainedModel'))\n",
        "print('Accelerate:', importlib.import_module('accelerate').__version__)\n",
        "try:\n",
        "    print('TRL:', importlib.import_module('trl').__version__)\n",
        "except Exception as e:\n",
        "    print('TRL import error:', e)\n",
        "try:\n",
        "    print('PEFT:', importlib.import_module('peft').__version__)\n",
        "except Exception as e:\n",
        "    print('PEFT import error:', e)\n",
        "print('BitsAndBytes present:', pkgutil.find_loader('bitsandbytes') is not None)\n",
        "print('\\nFirst few sys.path entries:')\n",
        "for p in sys.path[:8]:\n",
        "    print(' ', p)\n",
        "try:\n",
        "    from transformers import PreTrainedModel\n",
        "    print('Imported PreTrainedModel OK')\n",
        "except Exception as e:\n",
        "    print('PreTrainedModel import failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah-Yfr9nHa8W"
      },
      "outputs": [],
      "source": [
        "# STEPS:\n",
        "# TRAINING STEPS: INSTALL HUGGING FACE API, IMPORT MISTRAL 7B, RUN SFT TRAINER, SAVE MODEL, WE CAN UPDATE FOR RLHF LATER\n",
        "\n",
        "# DATACLEANING STEPS: REFORMAT PROMPT RESPONSE PAIRS INTO USABLE FORMAT FOR HUGGING FACE\n",
        "  # don't worry too much about file paths, etc. Just put in placeholders for now since I'm going to download the notebook and run it locally and I'll update it during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo_fQfVUIVql"
      },
      "outputs": [],
      "source": [
        "# this is how hugging face expects input output pairs, have script clean up provided datasets into this format\n",
        "<|im_start|>user # user just means user input\n",
        "{prompt}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant # assistant just means model output\n",
        "{output}\n",
        "<|im_end|>\n",
        "\n",
        "# example prompt\n",
        "<|im_start|>user\n",
        "Explain gradient descent.\n",
        "<|im_end|>\n",
        "\n",
        "# example output\n",
        "<|im_start|>assistant\n",
        "Gradient descent is an iterative optimization algorithm...\n",
        "<|im_end|>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version compiled:\", torch.version.cuda)\n",
        "print(\"cuDNN version:\", torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanjcLEVZIXs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, TaskType\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFfgItVJpWMI"
      },
      "outputs": [],
      "source": [
        "# 1. Configuration\n",
        "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
        "OUTPUT_DIR = \"./mistral_finetuned_v1\"\n",
        "\n",
        "# 2. Load Model with 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",        # Don't do model.to(...)\n",
        ")\n",
        "\n",
        "# 3. Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Fix for Mistral\n",
        "\n",
        "# 4. Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "# 5. Apply LoRA to the model\n",
        "from peft import get_peft_model\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()  # Debug info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "\n",
        "# Old tokens we want to strip out of the prompt/completion\n",
        "USER_TOKEN = \"<|im_start|>user\"\n",
        "ASSISTANT_TOKEN = \"<|im_start|>assistant\"\n",
        "END_TOKEN = \"<|im_end|>\"\n",
        "\n",
        "def load_json_files(json_dir):\n",
        "    \"\"\"\n",
        "    Loads all .json files in a directory and returns a list of dicts\n",
        "    with 'prompt' and 'completion' fields.\n",
        "    \"\"\"\n",
        "    json_dir = Path(json_dir)\n",
        "    examples = []\n",
        "\n",
        "    for file in json_dir.glob(\"*.json\"):\n",
        "        with open(file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            examples.extend(data)\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "def clean(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove legacy ChatML tokens from text and trim whitespace.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        text.replace(USER_TOKEN, \"\")\n",
        "            .replace(ASSISTANT_TOKEN, \"\")\n",
        "            .replace(END_TOKEN, \"\")\n",
        "            .strip()\n",
        "    )\n",
        "\n",
        "\n",
        "def merge_prompt_completion(example):\n",
        "    \"\"\"\n",
        "    Format dataset entries into correct Mistral instruction format.\n",
        "    \"\"\"\n",
        "    user = clean(example[\"prompt\"])\n",
        "    assistant = clean(example[\"completion\"])\n",
        "    return f\"<s>[INST] {user} [/INST] {assistant}</s>\"\n",
        "\n",
        "\n",
        "def build_mistral_dataset(json_dir):\n",
        "    raw = load_json_files(json_dir)\n",
        "    merged = [{\"text\": merge_prompt_completion(ex)} for ex in raw]\n",
        "    return Dataset.from_list(merged)\n",
        "\n",
        "\n",
        "# ======== USAGE EXAMPLE ========\n",
        "DATASET_DIRECTORY = \".\"   # change this to your directory\n",
        "\n",
        "dataset = build_mistral_dataset(DATASET_DIRECTORY)\n",
        "print(dataset[0])  # verify formatting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMD4jYuzZT9Y"
      },
      "outputs": [],
      "source": [
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    max_length=1024,          # max token length\n",
        "    dataset_text_field=\"text\" # your column name; default is already \"text\"\n",
        ")\n",
        "\n",
        "# 5. Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,              # your LoRA-wrapped Mistral\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,    # the Dataset you just built with \"text\" column\n",
        "    processing_class=tokenizer,  # <-- replaces `tokenizer=` in newer TRL\n",
        "    peft_config=None,         # you already did get_peft_model(), so no need here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UBUTDNLZW66"
      },
      "outputs": [],
      "source": [
        "# 6. Run Training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the cleaned and trimmed MassSynth dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"mistral_med_sft_scaled.jsonl\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    max_length=1024,          # max token length\n",
        "    dataset_text_field=\"text\" # your column name; default is already \"text\"\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,              # your LoRA-wrapped Mistral\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset['train'],    # the Dataset you just built with \"text\" column\n",
        "    processing_class=tokenizer,  # <-- replaces `tokenizer=` in newer TRL\n",
        "    peft_config=None,         # you already did get_peft_model(), so no need here\n",
        ")\n",
        "OUTPUT_DIR = \"./mistral_finetuned_v2\"\n",
        "# 6. Run Training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./mistral_finetuned_v2\\\\tokenizer_config.json',\n",
              " './mistral_finetuned_v2\\\\special_tokens_map.json',\n",
              " './mistral_finetuned_v2\\\\tokenizer.json')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trainer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
